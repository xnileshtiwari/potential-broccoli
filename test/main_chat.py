from llm.generative_model import get_completion
from Database.token_usage_database_update import update_token_usage
from pinecone_vector_database.query import pincone_vector_database_query  
from DASHBOARD.one_adder import increment_column_for_today
import os
# import streamlit as st
import logging

# Set up at the start of your application
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filename='app.log'
)



def start_chatting(index_name, user_input):
    increment_column_for_today(index_name)

    """
    Process the user input and return the response generated by the AI model.

    Parameters
    ----------
    index_name : str
        The name of the index to use for querying the vector database.
    user_input : str
        The user's input to process.

    Returns
    -------
    str
        The response generated by the AI model.
    """
    # Add debugging logs
    logging.debug(f"Attempting to query Pinecone with index: {index_name}")
    try:
        context = pincone_vector_database_query(user_input, index_name)
    except Exception as e:
        logging.error(f"Error querying Pinecone: {str(e)}")
        raise
    input_query = (f"""Case: {context}\n\n Question: {user_input}""")
    response, response_metadata = get_completion(input_query)

    
    input_token = response_metadata["input_tokens"]  # Input token
    output_token = response_metadata["output_tokens"] # Output token
    update_token_usage(input_token, output_token)  # Update token usage
    return response



